# Отчет по асинхронности и конкурентности

Этот отчет описывает реализацию асинхронной обработки и управления конкурентностью в проекте `free-llm-router-microservice`.

## 1. Общая архитектура асинхронности

Проект построен на базе **NestJS** с использованием адаптера **Fastify**. Вся обработка запросов выполняется в стандартном неблокирующем цикле событий Node.js (Event Loop).

### Ключевые компоненты:
*   **Контроллеры (`RouterController`)**: Все методы асинхронны (`async/await`). Для стриминга используется `AsyncGenerator`.
*   **Стриминг (SSE)**: Реализован через Server-Sent Events. Метод `chatCompletionStream` возвращает асинхронный итератор, который контроллер читает в цикле `for await` и пишет чанки в `res.raw` (raw socket fastify).
*   **Отмена запросов**: Используется стандартный Web API `AbortController` и `AbortSignal`. При обрыве соединения клиентом (`req.raw.on('close')`) срабатывает сигнал отмены, который прокидывается через все слои до провайдеров.

## 2. Параметр `maxConcurrent` (Удален)

**Статус**: Функциональность полностью удалена из проекта.

Ранее параметр использовался для ограничения параллельных запросов на уровне роутера, но был признан неэффективным из-за проблем с масштабированием (In-Memory State).
В текущей версии контроль конкурентности делегирован провайдерам API или внешним Rate Limiter'ам.

## 3. Аудит и выявленные проблемы

Ниже приведен список потенциальных проблем, архитектурных ограничений и особенностей реализации.

### 3.1. In-Memory State (Масштабируемость)
**Критичность: Высокая (при масштабировании)**
*   **Проблема**: `StateService` и `RateLimiterService` хранят состояние (Circuit Breaker, статистики, Token Buckets) в памяти процесса.
*   **Следствие**: Если запустить несколько экземпляров микросервиса (например, в Kubernetes с `replicas: 2`), лимиты `modelRequestsPerMinute` и состояние Circuit Breaker будут изолированы для каждого пода.
*   **Рекомендация**: Для горизонтального масштабирования необходимо выносить состояние в Redis.

### 3.2. Отсутствие очереди ожидания
**Критичность: Средняя**
*   **Проблема**: При достижении Rate Limit'ов (429 от провайдера или локального лимита) запрос отклоняется или ретраится, но нет механизма "постановки в очередь".
*   **Отсутствующая логика**: Нет механизма "подождать освобождения слота" (Queueing).

### 3.3. Логика ретраев при строгих Rate Limits
**Критичность: Низкая (зависит от настроек)**
*   **Анализ**: `RetryHandler` повторяет запрос к **той же модели** при ошибке 429 (Rate Limit). В `RateLimiterService` реализован алгоритм Token Bucket с линейным пополнением.
*   **Риск**: Если `modelRequestsPerMinute` очень мал (например, < 60), а `retryDelay` короткий (например, 1 сек), то бакет не успеет накопить даже 1 токен за время задержки. Ретраи будут расходоваться впустую.
*   **Рекомендация**: Учитывать скорость пополнения бакета при расчете `retryDelay` или сразу переключаться на следующую модель при 429, считая модель временно "перегруженной".

### 3.4. Race Conditions
**Критичность: Низкая**
*   **Анализ**: В текущей реализации Node.js код выполняется в одном потоке.

### 3.5. Глобальные настройки лимитов
**Наблюдение**: `RateLimiterService` берет глобальный `modelRequestsPerMinute` из конфига и применяет его ко всем моделям одинаково. Нет возможности задать разный RPM для разных моделей (например, для быстрых — 1000, для медленных — 100). Это ограничивает гибкость.

### 3.6. Graceful Shutdown
**Позитивный момент**: Реализован `ShutdownService` и отслеживание активных запросов. При завершении работы приложения сервис дожидается окончания текущих запросов или отменяет их корректно. Это хорошо проработанная часть логики.

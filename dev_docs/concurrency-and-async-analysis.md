# Отчет по асинхронности и конкурентности

Этот отчет описывает реализацию асинхронной обработки и управления конкурентностью в проекте `free-llm-router-microservice`.

## 1. Общая архитектура асинхронности

Проект построен на базе **NestJS** с использованием адаптера **Fastify**. Вся обработка запросов выполняется в стандартном неблокирующем цикле событий Node.js (Event Loop).

### Ключевые компоненты:
*   **Контроллеры (`RouterController`)**: Все методы асинхронны (`async/await`). Для стриминга используется `AsyncGenerator`.
*   **Стриминг (SSE)**: Реализован через Server-Sent Events. Метод `chatCompletionStream` возвращает асинхронный итератор, который контроллер читает в цикле `for await` и пишет чанки в `res.raw` (raw socket fastify).
*   **Отмена запросов**: Используется стандартный Web API `AbortController` и `AbortSignal`. При обрыве соединения клиентом (`req.raw.on('close')`) срабатывает сигнал отмены, который прокидывается через все слои до провайдеров.

## 2. Параметр `maxConcurrent`

Параметр управляет максимальным количеством одновременных активных запросов к конкретной модели.

### Где указывается:
1.  **`models.yaml`**: Свойство `maxConcurrent` у каждой модели (по умолчанию `Infinity`).
2.  **`config.yaml`**: В секции `modelOverrides` можно переопределить значение для конкретной модели без изменения `models.yaml`.

### Как работает:
1.  **Хранение состояния**: В `StateService` (in-memory) хранится счетчик `activeRequests` для каждой модели.
2.  **Проверка**: В `SmartStrategy` при выборе модели метод `hasCapacity` проверяет: `activeRequests < maxConcurrent`. Если условие не выполняется, модель исключается из списка кандидатов для текущего запроса.
3.  **Инкремент/Декремент**:
    *   Счетчик увеличивается (`incrementActiveRequests`) в `RouterService` **немедленно** после выбора модели, перед началом выполнения запроса к провайдеру.
    *   Счетчик уменьшается (`decrementActiveRequests`) в блоке `finally` после завершения запроса (успешного или ошибочного).

## 3. Аудит и выявленные проблемы

Ниже приведен список потенциальных проблем, архитектурных ограничений и особенностей реализации.

### 3.1. In-Memory State (Масштабируемость)
**Критичность: Высокая (при масштабировании)**
*   **Проблема**: `StateService` и `RateLimiterService` хранит счетчики активных запросов и бакеты лимитов в памяти процесса.
*   **Следствие**: Если запустить несколько экземпляров микросервиса (например, в Kubernetes с `replicas: 2`), лимиты `maxConcurrent` и `modelRequestsPerMinute` будут применяться **независимо** к каждому поду.
    *   При `maxConcurrent: 10` и 2 подах, реальная нагрузка на модель может достигать 20 запросов.
*   **Рекомендация**: Для горизонтального масштабирования необходимо выносить состояние (счетчики и rate limiters) в Redis.

### 3.2. Отсутствие очереди ожидания
**Критичность: Средняя**
*   **Проблема**: Если все подходящие модели достигли лимита `maxConcurrent`, `SmartStrategy` вернет `null` (нет подходящей модели). Запрос мгновенно упадет с ошибкой.
*   **Отсутствующая логика**: Нет механизма "подождать освобождения слота" (Queueing). Пользователь получает отказ вместо небольшой задержки.

### 3.3. Логика ретраев при строгих Rate Limits
**Критичность: Низкая (зависит от настроек)**
*   **Анализ**: `RetryHandler` повторяет запрос к **той же модели** при ошибке 429 (Rate Limit). В `RateLimiterService` реализован алгоритм Token Bucket с линейным пополнением.
*   **Риск**: Если `modelRequestsPerMinute` очень мал (например, < 60), а `retryDelay` короткий (например, 1 сек), то бакет не успеет накопить даже 1 токен за время задержки. Ретраи будут расходоваться впустую.
*   **Рекомендация**: Учитывать скорость пополнения бакета при расчете `retryDelay` или сразу переключаться на следующую модель при 429, считая модель временно "перегруженной".

### 3.4. Race Conditions
**Критичность: Низкая**
*   **Анализ**: В текущей реализации Node.js код между выбором модели и инкрементом счетчика выполняется синхронно. Состояния гонки внутри одного процесса нет.

### 3.5. Глобальные настройки лимитов
**Наблюдение**: `RateLimiterService` берет глобальный `modelRequestsPerMinute` из конфига и применяет его ко всем моделям одинаково. Нет возможности задать разный RPM для разных моделей (например, для быстрых — 1000, для медленных — 100). Это ограничивает гибкость.

### 3.6. Graceful Shutdown
**Позитивный момент**: Реализован `ShutdownService` и отслеживание активных запросов. При завершении работы приложения сервис дожидается окончания текущих запросов или отменяет их корректно. Это хорошо проработанная часть логики.

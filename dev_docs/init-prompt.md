давай спланируем микросервис роутера бесплатных llm, запиши план в dev_docs на русском языке.

на данный момент в репозитории находится бойлерплейт микросервиса на nestjs, нужно сделать микросервис на основе него.

в микросерисе будет несколько провайдеров, но на данный момент пока будет реализован только openrouter.

## глобальный список

будет список провайдеров, их llm и их параметров. этот список будет находиться в репозитории и по умолчанию будет загружаться при запуске микросервиса. но так же можно задать путь к своему списку или url чтобы скачать кактой-то готовый список при старте микросервиса.

модели в списке будут иметь следующие параметры:

- имя - упрощенное имя модели чтобы униерсально запрашиать ее у разных провайдеров где она может иметь специфичное имя
- model - реальное имя модели у провайдера
- скорость
- размер контекста
- тип - fast/reasoning
- тэги
- дополнительно значения лимитов для провайдеров с лимитами
- поддержка json ответа

так же предложи еще какието важные параметры по которым можно будет сделать авто выбор моделей, ну и обще нуные параметры.

## конфиг

в конфиге можно сконфигурировать провайдеры и лимиты для отдельных моделей, а так же другие опции вроде retries, timeout и т.д. И конечно будет указан файл или url для глобального списка моделей если нужно его переопределить

## переменые окружения

задаем путь к конфигу

## как происходит запрос

- делаем запрос на /(предложи имя эндпоинта)
- по умолчанию используется авто выбор из всех моделей, но можно указать тэги, минимальную скорость, тип или даже конкретную модель
- сервер получает параметры, резолвит на какую модель делать запрос и делает запрос на провайдера к которому относится модель
- получает ответ от провайдера и возвращает его в унифицированном виде

## авто выбор

- может быть несколько алгоритмов авто выбора
- по умолчанию round-robin
- в будущем можно вести статистику и делать выбор на основе статистики
- (предолжи еще варианты)

## fallback

- если запрос не удался, то далее выбирается следующая модель
- можно задать максимальное количество попыток и число смены моделей
- в ответе на запросе клиента будет указанна информация о сбоях, но сам по себе запрос для клиента будет выглядеть прозрачно, как будто запрос прошел успешно
- так же можно указать фолбэк на платную модель, туда будет запрос если будет исчерпан лимит перебора бесплатных моделей
- в будущем можно сохранять статистику и делать выбор на основе статистики, чтобы алгоритм сразу отсеивал плохие модели

## принцип работы

основная идея в том чтобы я мог сделать запрос например к deepseek R1 и мне не важно на каком провайдере от выполнится. но если запрос будет не удачен то пусть выполнится на платном провайдере.

## дополнительно

- простой vanilla ui для проверки моделей и их лимитов.


## уточнения

-  API совместим с OpenAI Chat Completions API
- аутентификация в микросерисе не нужна, она будет на уровне api gateway
- Streaming пока на будущее
- выпиши в отдельный список все что на будущее
- формат списка и конфига - YAML
- фолбэк будет только на одну платную модель, пока делаем поддержку провайдера deepseek и openrouter платных моделей. он будет срабатывать только в самом конце.
- maxOutputTokens - добавь
- алгоритмы предложил хорошие, пусть будут на потом
- оцени разумно ли добавить url для вебхука который будет дергаться если будут возникать не временные проблемы с моделями, например 404. или это лучше делать както через otel



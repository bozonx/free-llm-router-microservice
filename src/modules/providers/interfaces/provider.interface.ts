import type { Tool, ToolCall, ToolChoice } from './tools.interface.js';

/**
 * Chat message structure
 */
export interface ChatMessage {

  /**
   * Message role
   */
  role: 'system' | 'user' | 'assistant' | 'tool';

  /**
   * Message content
   */
  content: string | Array<{ type: string; text?: string; image_url?: { url: string; detail?: string } }> | null;

  /**
   * Optional message name (for function/tool messages)
   */
  name?: string;

  /**
   * Tool calls (for assistant messages)
   */
  tool_calls?: ToolCall[];

  /**
   * Tool call ID (for tool messages)
   */
  tool_call_id?: string;
}

/**
 * Chat completion request parameters
 */
export interface ChatCompletionParams {
  /**
   * Model ID to use
   */
  model: string;

  /**
   * Chat messages
   */
  messages: ChatMessage[];

  /**
   * Temperature (0-2)
   */
  temperature?: number;

  /**
   * Maximum tokens to generate
   */
  maxTokens?: number;

  /**
   * Top P sampling
   */
  topP?: number;

  /**
   * Frequency penalty
   */
  frequencyPenalty?: number;

  /**
   * Presence penalty
   */
  presencePenalty?: number;

  /**
   * Stop sequences
   */
  stop?: string | string[];

  /**
   * JSON mode enabled
   */
  jsonMode?: boolean;

  /**
   * Abort signal for request cancellation (graceful shutdown)
   */
  abortSignal?: AbortSignal;

  /**
   * List of tools available to the model
   */
  tools?: Tool[];

  /**
   * Tool choice constraint
   */
  toolChoice?: ToolChoice;

  /**
   * Enable streaming mode (Server-Sent Events)
   */
  stream?: boolean;

  /**
   * Request timeout in seconds (overrides provider default)
   */
  timeoutSecs?: number;
}

/**
 * Chat completion result
 */
export interface ChatCompletionResult {
  /**
   * Completion ID
   */
  id: string;

  /**
   * Model used
   */
  model: string;

  /**
   * Generated content (null when tool_calls are present)
   */
  content: string | null;

  /**
   * Finish reason
   */
  finishReason: 'stop' | 'length' | 'content_filter' | 'tool_calls';

  /**
   * Token usage statistics
   */
  usage: {
    promptTokens: number;
    completionTokens: number;
    totalTokens: number;
  };

  /**
   * Tool calls generated by the model
   */
  toolCalls?: ToolCall[];
}

/**
 * Chat completion stream chunk (SSE format)
 * Represents a single chunk in the streaming response
 */
export interface ChatCompletionStreamChunk {
  /**
   * Chunk ID
   */
  id: string;

  /**
   * Model used
   */
  model: string;

  /**
   * Delta content (incremental)
   */
  delta: {
    /**
     * Role (only in first chunk)
     */
    role?: 'assistant';

    /**
     * Content chunk
     */
    content?: string;

    /**
     * Tool calls delta (for function calling)
     */
    tool_calls?: ToolCall[];
  };

  /**
   * Finish reason (only in last chunk)
   */
  finishReason?: 'stop' | 'length' | 'content_filter' | 'tool_calls';

  /**
   * Router metadata (optional, included in first or last chunk)
   */
  _router?: {
    provider: string;
    model_name: string;
    attempts?: number;
    fallback_used?: boolean;
  };
}

/**
 * LLM provider interface
 */
export interface LlmProvider {
  /**
   * Provider name
   */
  readonly name: string;

  /**
   * Perform chat completion
   */
  chatCompletion(params: ChatCompletionParams): Promise<ChatCompletionResult>;

  /**
   * Perform chat completion with streaming (SSE)
   * Returns an async generator that yields chunks
   */
  chatCompletionStream(
    params: ChatCompletionParams,
  ): AsyncGenerator<ChatCompletionStreamChunk, void, unknown>;
}

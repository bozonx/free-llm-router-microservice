# Path to models file (optional, defaults to ./models.yaml)
modelsFile: ./models.yaml

# Provider settings
# Note: Use ${ENV_VAR_NAME} to reference any environment variable from .env file
providers:
  openrouter:
    enabled: true
    apiKey: ${OPENROUTER_API_KEY}
    baseUrl: https://openrouter.ai/api/v1
    
  deepseek:
    enabled: true
    apiKey: ${DEEPSEEK_API_KEY}
    baseUrl: https://api.deepseek.com

# Routing settings
routing:
  # Maximum retries on free models
  maxRetries: 3
  
  # Maximum retries on 429 rate limit for one model
  rateLimitRetries: 2
  
  # Delay between retries (ms) - only for rate limit (429) retries on the same model
  retryDelay: 1000
  # Note: Jitter ±20% is hardcoded in RETRY_JITTER_PERCENT constant
  
  # Provider request timeout (ms)
  timeoutSecs: 30 # - в секундах
  
  # Fallback to paid model
  fallback:
    enabled: true
    provider: deepseek        # or openrouter
    model: deepseek-chat      # model for fallback

# Circuit Breaker settings (optional, has defaults)
# circuitBreaker:
#   failureThreshold: 3       # Failures to open circuit (default: 3)
#   cooldownPeriodSecs: 60    # Cooldown in seconds (default: 60 = 1 min)
#   successThreshold: 2       # Successes to close from HALF_OPEN (default: 2)
#   statsWindowSizeMins: 5    # Statistics window in minutes (default: 5 mins)

# Model priority/weight overrides (optional)
# Allows adjusting priorities without editing models.yaml
# modelOverrides:
#   - name: llama-3.3-70b
#     priority: 2             # Lower priority (higher number)
#     weight: 5               # Lower weight
#     maxConcurrent: 10       # Limit concurrency
#   - name: deepseek-r1
#     provider: deepseek      # Optional validation
#     priority: 1             # Higher priority
#     weight: 15              # Higher weight
#     available: false        # Temporarily disable

# Rate limiting settings (optional, disabled by default)
# rateLimiting:
#   enabled: false            # Enable rate limiting
#   
#   # Global limit (all clients combined)
#   global:
#     requestsPerMinute: 100  # Max requests per minute for all clients
#     
#   # Per-client limit (by X-Client-ID header or IP)
#   perClient:
#     enabled: true
#     requestsPerMinute: 20   # Max requests per minute per client
#     burstSize: 5            # Extra tokens for burst traffic
#     
#   # Per-model limit (protection against skew)
#   perModel:
#     enabled: true
#     requestsPerMinute: 30   # Max requests per minute per model
